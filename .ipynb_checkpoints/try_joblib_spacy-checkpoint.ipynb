{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c38a54f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymusas.spacy_api.taggers.rule_based.RuleBasedTagger at 0x7fe99bb76b40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# We exclude the following components as we do not need them. \n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser', 'ner'])\n",
    "# Load the English PyMUSAS rule based tagger in a separate spaCy pipeline\n",
    "english_tagger_pipeline = spacy.load('en_dual_none_contextual')\n",
    "# Adds the English PyMUSAS rule based tagger to the main spaCy pipeline\n",
    "nlp.add_pipe('pymusas_rule_based_tagger', source=english_tagger_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2326354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr\tMr\tPROPN\t['Z1m']\n",
      "John\tJohn\tPROPN\t['Z1m']\n",
      "Dalton\tDalton\tPROPN\t['Z1m']\n"
     ]
    }
   ],
   "source": [
    "token = nlp('Mr John Dalton')\n",
    "for t in token:\n",
    "    print(f'{t.text}\\t{t.lemma_}\\t{t.pos_}\\t{t._.pymusas_tags}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0539292d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\tthe\tDET\t['Z5']\n",
      "Nile\tNile\tPROPN\t['Z2']\n",
      "is\tbe\tAUX\t['A3+', 'Z5']\n",
      "a\ta\tDET\t['Z5']\n",
      "major\tmajor\tADJ\t['A11.1+', 'N3.2+']\n",
      "north\tnorth\tNOUN\t['M6']\n",
      "-\t-\tPUNCT\t['PUNCT']\n",
      "flowing\tflow\tVERB\t['M4', 'M1']\n",
      "river\triver\tNOUN\t['W3/M4', 'N5+']\n",
      "in\tin\tADP\t['Z5']\n",
      "Northeastern\tNortheastern\tPROPN\t['Z1mf', 'Z3c']\n",
      "Africa\tAfrica\tPROPN\t['Z1mf', 'Z3c']\n",
      ".\t.\tPUNCT\t['PUNCT']\n",
      "The\tthe\tDET\t['Z5']\n",
      "Nile\tNile\tPROPN\t['Z2']\n",
      "is\tbe\tAUX\t['A3+', 'Z5']\n",
      "a\ta\tDET\t['Z5']\n",
      "major\tmajor\tADJ\t['A11.1+', 'N3.2+']\n",
      "north\tnorth\tNOUN\t['M6']\n",
      "-\t-\tPUNCT\t['PUNCT']\n",
      "flowing\tflow\tVERB\t['M4', 'M1']\n",
      "river\triver\tNOUN\t['W3/M4', 'N5+']\n",
      "in\tin\tADP\t['Z5']\n",
      "Northeastern\tNortheastern\tPROPN\t['Z1mf', 'Z3c']\n",
      "Africa\tAfrica\tPROPN\t['Z1mf', 'Z3c']\n",
      ".\t.\tPUNCT\t['PUNCT']\n",
      "CPU times: user 41.3 ms, sys: 2.67 ms, total: 44 ms\n",
      "Wall time: 42.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "texts = ['The Nile is a major north-flowing river in Northeastern Africa.',\n",
    "         'The Nile is a major north-flowing river in Northeastern Africa.',\n",
    "        'The Nile is a major north-flowing river in Northeastern Africa.',\n",
    "        'The Nile is a major north-flowing river in Northeastern Africa.']\n",
    "\n",
    "def process_text(text):\n",
    "    token = nlp(text)\n",
    "    for t in token:\n",
    "        print(f'{t.text}\\t{t.lemma_}\\t{t.pos_}\\t{t._.pymusas_tags}')\n",
    "\n",
    "number_of_cpu = joblib.cpu_count()\n",
    "\n",
    "delayed_funcs = [delayed(process_text)(text) for text in texts]\n",
    "parallel_pool = Parallel(n_jobs=1, prefer='processes')\n",
    "\n",
    "%time parallel_pool(delayed_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8fb28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"Flatten a list of lists to a combined list\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def process_chunk(texts):\n",
    "    preproc_pipe = []\n",
    "    for doc in nlp.pipe(texts, batch_size=20):\n",
    "        preproc_pipe.append(lemmatize_pipe(doc))\n",
    "    return preproc_pipe\n",
    "\n",
    "def preprocess_parallel(texts, chunksize=100):\n",
    "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(process_chunk)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, len(df_preproc), chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4982f451",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_preproc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mpreprocess_parallel\u001b[0;34m(texts, chunksize)\u001b[0m\n\u001b[1;32m     17\u001b[0m executor \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m do \u001b[38;5;241m=\u001b[39m delayed(process_chunk)\n\u001b[0;32m---> 19\u001b[0m tasks \u001b[38;5;241m=\u001b[39m (do(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker(texts, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdf_preproc\u001b[49m), chunksize\u001b[38;5;241m=\u001b[39mchunksize))\n\u001b[1;32m     20\u001b[0m result \u001b[38;5;241m=\u001b[39m executor(tasks)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flatten(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_preproc' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "preprocess_parallel(texts, chunksize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd66ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
